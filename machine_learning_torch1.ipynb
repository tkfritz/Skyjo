{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821fc6b1",
   "metadata": {},
   "source": [
    "In this notebook I am using pytorch for deep learning. It iuses the original data set which is generated with a rather random playing computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffe7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard modukles\n",
    "import numpy as np\n",
    "import random as random\n",
    "import time\n",
    "import pandas as pd\n",
    "#using simpleguitk for display, is not needed for computer game\n",
    "#likely not needed un this notebook \n",
    "import simpleguitk as simplegui\n",
    "#for plotting \n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "#for efficient saving and loading\n",
    "import pickle\n",
    "#skyjp game classes and functions\n",
    "from skyjo_functions4 import *\n",
    "#own functions for machine learning\n",
    "from ml_functions2 import *\n",
    "#for machine learning\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#likely some uneeded ijmported\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "#time logging\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "#again tensorflow problem work before with conda install cudatoolkit, is not needed after each restart? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92f0dd",
   "metadata": {},
   "source": [
    "Below I create the column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b24a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['open_pile_card', 'own_cards_0', 'own_cards_1', 'own_cards_2', 'own_cards_3', 'own_cards_4', 'own_cards_5', 'own_cards_6', 'own_cards_7', 'own_cards_8', 'own_cards_9', 'own_cards_10', 'own_cards_11', 'other_player_cards_0', 'other_player_cards_1', 'other_player_cards_2', 'other_player_cards_3', 'other_player_cards_4', 'other_player_cards_5', 'other_player_cards_6', 'other_player_cards_7', 'other_player_cards_8', 'other_player_cards_9', 'other_player_cards_10', 'other_player_cards_11', 'action_take_open', 'action_discard', 'discard_value', 'id_player_card', 'numeric_player_card', 'score_self', 'score_other', 'round']\n",
      "The first 30 columns are the features\n",
      "['open_pile_card', 'own_cards_0', 'own_cards_1', 'own_cards_2', 'own_cards_3', 'own_cards_4', 'own_cards_5', 'own_cards_6', 'own_cards_7', 'own_cards_8', 'own_cards_9', 'own_cards_10', 'own_cards_11', 'other_player_cards_0', 'other_player_cards_1', 'other_player_cards_2', 'other_player_cards_3', 'other_player_cards_4', 'other_player_cards_5', 'other_player_cards_6', 'other_player_cards_7', 'other_player_cards_8', 'other_player_cards_9', 'other_player_cards_10', 'other_player_cards_11', 'action_take_open', 'action_discard', 'discard_value', 'id_player_card', 'numeric_player_card']\n",
      "The last three are not features. The differential of the first two is the target andthe last is the ignored row index\n",
      "['score_self', 'score_other', 'round']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns=['open_pile_card']\n",
    "for i in range(12):\n",
    "    columns.append('own_cards_'+str(i))\n",
    "for i in range(12):\n",
    "    columns.append('other_player_cards_'+str(i))    \n",
    "columns.append('action_take_open')\n",
    "columns.append('action_discard')\n",
    "columns.append('discard_value')\n",
    "columns.append('id_player_card')\n",
    "columns.append('numeric_player_card')            \n",
    "columns.append('score_self')\n",
    "columns.append('score_other')\n",
    "columns.append('round')\n",
    "print(columns)\n",
    "print(\"The first 30 columns are the features\")\n",
    "print(columns[0:30])\n",
    "print(\"The last three are not features. The differential of the first two is the target andthe last is the ignored row index\")\n",
    "print(columns[30:33])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f05739",
   "metadata": {},
   "source": [
    "Three different data sets are combined to one. All columns are used here, since the neural net should be able \n",
    "to ignore unimportant ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6db49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600000, 33) (3000000, 33)\n"
     ]
    }
   ],
   "source": [
    "df1=pd.read_pickle('level_rand_v2_0_600000_0.pkl')\n",
    "df2=pd.read_pickle('level_rand_v2_0_600000_1.pkl')\n",
    "df3=pd.read_pickle('level_rand_v2_0_600000_2.pkl')\n",
    "df4=pd.read_pickle('level_rand_v2_0_600000_3.pkl')\n",
    "df5=pd.read_pickle('level_rand_v2_0_600000_4.pkl')\n",
    "df=pd.concat([df1,df2,df3,df4,df5],join='outer', ignore_index=True)\n",
    "print(df1.shape,df.shape)\n",
    "#free space\n",
    "df1=0\n",
    "df2=0\n",
    "df3=0\n",
    "df4=0\n",
    "df5=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a1b1a",
   "metadata": {},
   "source": [
    "Now I divide into test, train and validation and also into features and target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9573e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500000\n",
      "2400000\n",
      "(1500000, 30) (900000, 30) (900000,)    open_pile_card  own_cards_0  own_cards_1  own_cards_2  own_cards_3  \\\n",
      "0              -2           20            7           20           20   \n",
      "1               0           -2            1            1           11   \n",
      "2              -1           20           20           20           20   \n",
      "3              11           11           -2            0           20   \n",
      "4               4           -1            0            3            0   \n",
      "\n",
      "   own_cards_4  own_cards_5  own_cards_6  own_cards_7  own_cards_8  ...  \\\n",
      "0            5           20            2           20            8  ...   \n",
      "1            2           20           -1            6            7  ...   \n",
      "2           20           20           20            0           20  ...   \n",
      "3           20            8           -1           20           12  ...   \n",
      "4            5           20           20            4            0  ...   \n",
      "\n",
      "   other_player_cards_7  other_player_cards_8  other_player_cards_9  \\\n",
      "0                    20                     3                    20   \n",
      "1                    20                    20                    -1   \n",
      "2                     9                    20                    20   \n",
      "3                     9                     1                    20   \n",
      "4                     4                     0                     5   \n",
      "\n",
      "   other_player_cards_10  other_player_cards_11  action_take_open  \\\n",
      "0                     20                     10                 1   \n",
      "1                      4                      9                 0   \n",
      "2                     20                     20                 0   \n",
      "3                      0                     20                 0   \n",
      "4                      7                     12                 0   \n",
      "\n",
      "   action_discard  discard_value  id_player_card  numeric_player_card  \n",
      "0               0             30               8                    8  \n",
      "1               1             12               5                   20  \n",
      "2               1             10               0                   20  \n",
      "3               0              6              11                   10  \n",
      "4               1             10               5                   20  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_train,feature_test,feature_valid, target_train,target_test,target_valid=split_test_valid(df.iloc[:,0:30],df.score_self-df.score_other,0.5,0.8)\n",
    "print(feature_train.shape,feature_test.shape,target_test.shape,feature_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d78d3c",
   "metadata": {},
   "source": [
    "Here is xgboost for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6161dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum standard deviation of (prediction-test data) 57.6014 of max-depth=5\n",
      "minimum standard deviation of (prediction-test data) 57.5013 of max-depth=6\n",
      "minimum standard deviation of (prediction-test data) 57.4394 of max-depth=7\n",
      "minimum standard deviation of (prediction-test data) 57.4389 of max-depth=8\n",
      "minimum standard deviation of (prediction-test data) 57.4242 of max-depth=9\n",
      "minimum standard deviation of (prediction-test data) 57.4588 of max-depth=10\n"
     ]
    }
   ],
   "source": [
    "resd=np.loadtxt('xgb_v2_maxd5.txt')\n",
    "rese=np.loadtxt('xgb_v2_maxd6.txt')\n",
    "resf=np.loadtxt('xgb_v2_maxd7.txt')\n",
    "resg=np.loadtxt('xgb_v2_maxd8.txt')\n",
    "resh=np.loadtxt('xgb_v2_maxd9.txt')\n",
    "resi=np.loadtxt('xgb_v2_maxd10.txt')\n",
    "print(f\"minimum standard deviation of (prediction-test data) {round(min(resd[3]),4)} of max-depth=5\")\n",
    "print(f\"minimum standard deviation of (prediction-test data) {round(min(rese[3]),4)} of max-depth=6\")\n",
    "print(f\"minimum standard deviation of (prediction-test data) {round(min(resf[3]),4)} of max-depth=7\")\n",
    "print(f\"minimum standard deviation of (prediction-test data) {round(min(resg[3]),4)} of max-depth=8\")\n",
    "print(f\"minimum standard deviation of (prediction-test data) {round(min(resh[3]),4)} of max-depth=9\")\n",
    "print(f\"minimum standard deviation of (prediction-test data) {round(min(resi[3]),4)} of max-depth=10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007a98a",
   "metadata": {},
   "source": [
    "Best standard deviation is 57.4242.\n",
    "\n",
    "Now I create the first class for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd921147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1ee55",
   "metadata": {},
   "source": [
    "Here I convert the pendas data frames to numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cdf4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_train, target_train = np.array(feature_train), np.array(target_train)\n",
    "feature_test, target_test = np.array(feature_test), np.array(target_test)\n",
    "feature_valid, target_valid = np.array(feature_valid), np.array(target_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35299ce",
   "metadata": {},
   "source": [
    "Now I concat features and targets as needed by torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8bbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use int causes error later because wieghts are floats\n",
    "train_dataset = RegressionDataset(torch.from_numpy(feature_train).float(), torch.from_numpy(target_train).float())\n",
    "test_dataset = RegressionDataset(torch.from_numpy(feature_test).float(), torch.from_numpy(target_test).float())\n",
    "valid_dataset = RegressionDataset(torch.from_numpy(feature_valid).float(), torch.from_numpy(target_valid).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c66e7",
   "metadata": {},
   "source": [
    "This are the parameters needed, I mainly varied batch_size, but I am still not sure what is best in this cases of many rows and not so many columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5166da",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cef2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valid_dataset, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45d0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96a742b0",
   "metadata": {},
   "source": [
    "I am starting with a dense network of just two layers, thus just one layer more than the single layer of linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5310f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleRegression(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(MultipleRegression, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 10)\n",
    "        self.layer_out = nn.Linear(10, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)\n",
    "    def predict(self, test_inputs):\n",
    "        x = self.relu(self.layer_1(test_inputs))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "639a72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleRegression2(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(MultipleRegression2, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 20)\n",
    "        self.layer_2 = nn.Linear(20, 10)        \n",
    "        self.layer_out = nn.Linear(10, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)\n",
    "    def predict(self, test_inputs):\n",
    "        x = self.relu(self.layer_1(test_inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def1fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now 3 layers \n",
    "class MultipleRegression3(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(MultipleRegression3, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_features, 40)\n",
    "        self.layer_2 = nn.Linear(40, 20)  \n",
    "        self.layer_3 = nn.Linear(20, 8)        \n",
    "        self.layer_out = nn.Linear(8, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.layer_out(x)\n",
    "        return (x)\n",
    "    def predict(self, test_inputs):\n",
    "        x = self.relu(self.layer_1(test_inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.relu(self.layer_3(x))        \n",
    "        x = self.layer_out(x)\n",
    "        return (x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c18ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "###################### OUTPUT ######################cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c4d684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleRegression(\n",
      "  (layer_1): Linear(in_features=30, out_features=10, bias=True)\n",
      "  (layer_out): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultipleRegression(NUM_FEATURES)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bee2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca3c061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a73116998ba41fa858fb56eb44002bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 4105.55947 | Val Loss: 4083.99538\n",
      "Epoch 002: | Train Loss: 4062.55707 | Val Loss: 4046.96144\n",
      "Epoch 003: | Train Loss: 4036.13904 | Val Loss: 4030.64457\n",
      "Epoch 004: | Train Loss: 4022.85640 | Val Loss: 4018.98036\n",
      "Epoch 005: | Train Loss: 4011.00434 | Val Loss: 4008.88819\n",
      "Epoch 006: | Train Loss: 4001.88734 | Val Loss: 3999.47147\n",
      "Epoch 007: | Train Loss: 3995.42135 | Val Loss: 3994.25424\n",
      "Epoch 008: | Train Loss: 3989.06651 | Val Loss: 3985.72731\n",
      "Epoch 009: | Train Loss: 3980.45235 | Val Loss: 3982.41948\n",
      "Epoch 010: | Train Loss: 3972.78714 | Val Loss: 3968.35652\n",
      "Epoch 011: | Train Loss: 3957.89985 | Val Loss: 3953.70244\n",
      "Epoch 012: | Train Loss: 3947.10417 | Val Loss: 3944.42325\n",
      "Epoch 013: | Train Loss: 3941.40366 | Val Loss: 3940.15116\n",
      "Epoch 014: | Train Loss: 3936.98633 | Val Loss: 3936.35596\n",
      "Epoch 015: | Train Loss: 3933.76444 | Val Loss: 3932.97289\n",
      "Epoch 016: | Train Loss: 3930.44774 | Val Loss: 3929.28759\n",
      "Epoch 017: | Train Loss: 3928.12622 | Val Loss: 3928.04676\n",
      "Epoch 018: | Train Loss: 3926.00578 | Val Loss: 3925.16602\n",
      "Epoch 019: | Train Loss: 3924.21856 | Val Loss: 3922.77391\n",
      "Epoch 020: | Train Loss: 3922.35160 | Val Loss: 3921.84132\n",
      "Epoch 021: | Train Loss: 3920.94338 | Val Loss: 3918.99297\n",
      "Epoch 022: | Train Loss: 3920.01911 | Val Loss: 3920.16652\n",
      "Epoch 023: | Train Loss: 3918.52621 | Val Loss: 3916.48286\n",
      "Epoch 024: | Train Loss: 3917.57153 | Val Loss: 3916.38808\n",
      "Epoch 025: | Train Loss: 3916.54320 | Val Loss: 3922.44545\n",
      "Epoch 026: | Train Loss: 3915.74590 | Val Loss: 3914.35058\n",
      "Epoch 027: | Train Loss: 3914.38713 | Val Loss: 3915.28959\n",
      "Epoch 028: | Train Loss: 3913.58706 | Val Loss: 3915.99166\n",
      "Epoch 029: | Train Loss: 3913.08309 | Val Loss: 3913.29640\n",
      "Epoch 030: | Train Loss: 3911.70110 | Val Loss: 3911.88412\n",
      "Epoch 031: | Train Loss: 3911.47030 | Val Loss: 3908.46497\n",
      "Epoch 032: | Train Loss: 3910.86832 | Val Loss: 3909.49352\n",
      "Epoch 033: | Train Loss: 3910.46071 | Val Loss: 3907.21637\n",
      "Epoch 034: | Train Loss: 3909.16308 | Val Loss: 3908.16732\n",
      "Epoch 035: | Train Loss: 3908.55582 | Val Loss: 3908.70919\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m val_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_val_batch, y_val_batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     32\u001b[0m     X_val_batch, y_val_batch \u001b[38;5;241m=\u001b[39m X_val_batch\u001b[38;5;241m.\u001b[39mto(device), y_val_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m     y_val_pred \u001b[38;5;241m=\u001b[39m model(X_val_batch)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m, in \u001b[0;36mRegressionDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_data \u001b[38;5;241m=\u001b[39m X_data\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_data \u001b[38;5;241m=\u001b[39m y_data\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_data[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_data[index]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m (\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#slow with 64 batches , likely larger better as for tensorflow\n",
    "#10000 at 11:36 started 11:56 epoch 12 mse 4019 4023\n",
    "#100000 at 11:57 started  maybe faster but improvement is slower 12:12 epoch 10 4107 and 4108\n",
    "#1024 at 12:15 13:15 validation getsbest was 3907\n",
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804e3b8",
   "metadata": {},
   "source": [
    "Is slower thann tensorflow. Likely not optimal use of it. \n",
    "\n",
    "The measure is the squered standard deviation, that means the best xgboost had an MSE of 3297.54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7abbc9",
   "metadata": {},
   "source": [
    "As the next model I add one more layer to it to now two laters before the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99237fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#should improve at least reduce match size \n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = 30\n",
    "#depends on batch_size thus to be redone \n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valid_dataset, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e553ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleRegression2(\n",
      "  (layer_1): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (layer_2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (layer_out): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultipleRegression2(NUM_FEATURES)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae5431a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29df7e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6deed043243b4adfab9bf7a911af23a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 4088.17872 | Val Loss: 4042.49844\n",
      "Epoch 002: | Train Loss: 4003.61026 | Val Loss: 3975.01990\n",
      "Epoch 003: | Train Loss: 3941.41271 | Val Loss: 3912.77088\n",
      "Epoch 004: | Train Loss: 3893.41278 | Val Loss: 3879.09932\n",
      "Epoch 005: | Train Loss: 3855.28977 | Val Loss: 3839.36777\n",
      "Epoch 006: | Train Loss: 3825.19496 | Val Loss: 3811.99850\n",
      "Epoch 007: | Train Loss: 3801.53642 | Val Loss: 3796.07788\n",
      "Epoch 008: | Train Loss: 3780.38991 | Val Loss: 3769.23003\n",
      "Epoch 009: | Train Loss: 3760.36392 | Val Loss: 3753.23314\n",
      "Epoch 010: | Train Loss: 3746.01161 | Val Loss: 3749.13802\n",
      "Epoch 011: | Train Loss: 3733.94495 | Val Loss: 3738.42893\n",
      "Epoch 012: | Train Loss: 3720.72332 | Val Loss: 3715.80087\n",
      "Epoch 013: | Train Loss: 3712.73223 | Val Loss: 3716.50719\n",
      "Epoch 014: | Train Loss: 3705.49096 | Val Loss: 3704.24035\n",
      "Epoch 015: | Train Loss: 3698.89745 | Val Loss: 3697.26908\n",
      "Epoch 016: | Train Loss: 3694.99906 | Val Loss: 3699.80824\n",
      "Epoch 017: | Train Loss: 3689.02381 | Val Loss: 3695.67526\n",
      "Epoch 018: | Train Loss: 3684.23971 | Val Loss: 3687.52807\n",
      "Epoch 019: | Train Loss: 3681.94367 | Val Loss: 3680.51998\n",
      "Epoch 020: | Train Loss: 3678.50653 | Val Loss: 3672.33064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m val_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_val_batch, y_val_batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     28\u001b[0m     X_val_batch, y_val_batch \u001b[38;5;241m=\u001b[39m X_val_batch\u001b[38;5;241m.\u001b[39mto(device), y_val_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m     y_val_pred \u001b[38;5;241m=\u001b[39m model(X_val_batch)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/profiler.py:485\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[0;34m(self, name, args)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#two layers model \n",
    "#start about 15:27 is better, one layer seems never enough, 16:01 epoch 20 3672 not enough still clear improvement\n",
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d43db0",
   "metadata": {},
   "source": [
    "Now 3 layers and less epochs at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32381310",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = 30\n",
    "#depends on batch_size thus to be redone \n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valid_dataset, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d99a641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleRegression3(\n",
      "  (layer_1): Linear(in_features=30, out_features=40, bias=True)\n",
      "  (layer_2): Linear(in_features=40, out_features=20, bias=True)\n",
      "  (layer_3): Linear(in_features=20, out_features=8, bias=True)\n",
      "  (layer_out): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#load after 30 epochs \n",
    "PATH='/home/tobias/ml-testing/games/skyjo/mlp_3layers_noreg_30epochs.pkl'\n",
    "model =MultipleRegression3(NUM_FEATURES)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ae063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": [],\n",
    "     \"test\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f397761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870aa616bed449e99a5b762632d69670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now faster converging starting again at 30 epochs, should save all outputs\n",
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))  \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        test_epoch_loss = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for X_test_batch, y_test_batch in test_loader:\n",
    "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "            \n",
    "            y_test_pred = model(X_test_batch)\n",
    "                        \n",
    "            test_loss = criterion(y_test_pred, y_test_batch.unsqueeze(1))\n",
    "            \n",
    "            test_epoch_loss += test_loss.item()\n",
    "    loss_stats['test'].append(test_epoch_loss/len(test_loader))\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Test Loss: {test_epoch_loss/len(test_loader):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f8346e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard deviation after 10 epochs\n",
      "train 58.78522722589409\n",
      "validation 58.70091532506115\n",
      "\n",
      "The standard deviation after 20 epochs\n",
      "train 57.76159788302259\n",
      "validation 57.62012391517394\n",
      "\n",
      "The standard deviation after 30 epochs\n",
      "train 57.49837075952674\n",
      "validation 57.37565337318609\n",
      "\n",
      "The standard deviation after 40 epochs\n",
      "train 57.32865697014016\n",
      "validation 57.600369009234655\n"
     ]
    }
   ],
   "source": [
    "print(\"The standard deviation after 10 epochs\")\n",
    "print(f\"train {np.sqrt(3455.70294)}\")\n",
    "print(f\"validation {np.sqrt(3445.79746)}\\n\")\n",
    "\n",
    "print(\"The standard deviation after 20 epochs\")\n",
    "print(f\"train {np.sqrt(3336.40219)}\")\n",
    "print(f\"validation {np.sqrt(3320.07868)}\\n\")\n",
    "\n",
    "print(\"The standard deviation after 30 epochs\")\n",
    "print(f\"train {np.sqrt(3306.06264)}\")\n",
    "print(f\"validation {np.sqrt(3291.96560)}\\n\")\n",
    "\n",
    "print(\"The standard deviation after 40 epochs\")\n",
    "print(f\"train {np.sqrt(3286.57491)}\")\n",
    "print(f\"validation {np.sqrt(3317.80251)}/n\")\n",
    "\n",
    "#print(\"The standard deviation after 50 epochs\")\n",
    "#print(f\"train: {np.sqrt()}\")\n",
    "#print(f\"validation: {np.sqrt()}\")\n",
    "#print(f\"test: {np.sqrt()}/n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9244a3c",
   "metadata": {},
   "source": [
    "For comparison the standard deviation of the best xgboost prediction is 57.4242. After 20 epochs it gets close to it. After 30 it is better, slightly for vailidation but it also depends on the sample, and I used test before for it. Still it is of the same order now at lest. \n",
    "After 30 epochs oscillations does not anymore always improve, but still usually.\n",
    "\n",
    "The model looks now good enough that I save it.\n",
    "\n",
    "After 40 it looks less good it could be just chance, after most step it improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf5ff890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH='/home/tobias/ml-testing/games/skyjo/mlp_3layers_noreg_30epochs.pkl'\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02f3a11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultipleRegression3(\n",
       "  (layer_1): Linear(in_features=30, out_features=40, bias=True)\n",
       "  (layer_2): Linear(in_features=40, out_features=20, bias=True)\n",
       "  (layer_3): Linear(in_features=20, out_features=8, bias=True)\n",
       "  (layer_out): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading needs class\n",
    "PATH='/home/tobias/ml-testing/games/skyjo/mlp_3layers_noreg_30epochs.pkl'\n",
    "model2 =MultipleRegression3(NUM_FEATURES)\n",
    "model2.load_state_dict(torch.load(PATH))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d82e5",
   "metadata": {},
   "source": [
    "Saving seems to have worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d34eeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12:06 start 12 08 end \n",
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_list.append(y_test_pred.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc31d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in test\n",
      "after 30 epochs\n",
      "Mean Squared Error : 3302.0495686974614\n",
      "R^2 : 0.20235450378570075\n",
      "standard deviation : 57.45648483870122\n",
      "\n",
      "after 40 epochs\n",
      "Mean Squared Error : 3326.295089276\n",
      "R^2 : 0.19649773819496774\n",
      "standard deviation : 57.32872562676442\n"
     ]
    }
   ],
   "source": [
    "print(\"in test\")\n",
    "mse = mean_squared_error(target_test, y_pred_list)\n",
    "r_square = r2_score(target_test, y_pred_list)\n",
    "std=np.std(target_test-y_pred_list)\n",
    "print(\"after 30 epochs\")\n",
    "print(\"Mean Squared Error : 3302.0495686974614\")\n",
    "print(\"R^2 : 0.20235450378570075\")\n",
    "print(\"standard deviation : 57.45648483870122\")\n",
    "print(\"\")\n",
    "print(\"after 40 epochs\")\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"R^2 :\",r_square)\n",
    "print(\"standard deviation :\",std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd654f",
   "metadata": {},
   "source": [
    "After 30 epochs the test standard deviation is 57.4565 slightly worse than the best xgboost case of 57.4242. \n",
    "After the standard deviation is better but not the MSE. Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c8156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
